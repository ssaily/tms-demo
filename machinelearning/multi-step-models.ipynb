{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "continent-republican",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import datetime\n",
    "from pathlib import Path\n",
    "\n",
    "import IPython\n",
    "import IPython.display\n",
    "\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import tensorflow as tf\n",
    "\n",
    "import matplotlib as mpl\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "historical-citizen",
   "metadata": {},
   "outputs": [],
   "source": [
    "def wind_vector(self, velocity, max_velocity, direction):\n",
    "    # Convert to radians.\n",
    "    wd_rad = direction*np.pi / 180\n",
    "    self['Wx'] = velocity*np.cos(wd_rad)\n",
    "    self['Wy'] = velocity*np.sin(wd_rad)\n",
    "    self['max Wx'] = max_velocity*np.cos(wd_rad)\n",
    "    self['max Wy'] = max_velocity*np.sin(wd_rad)\n",
    "pd.DataFrame.wind_vector = wind_vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "informed-perspective",
   "metadata": {},
   "outputs": [],
   "source": [
    "def tod_signal(self, date_time):\n",
    "    day = 24*60*60\n",
    "    year = (365.2425)*day\n",
    "    timestamp_s = date_time.map(datetime.datetime.timestamp)\n",
    "    self['Day sin'] = np.sin(timestamp_s * (2 * np.pi / day))\n",
    "    self['Day cos'] = np.cos(timestamp_s * (2 * np.pi / day))\n",
    "    self['Year sin'] = np.sin(timestamp_s * (2 * np.pi / year))\n",
    "    self['Year cos'] = np.cos(timestamp_s * (2 * np.pi / year))\n",
    "pd.DataFrame.tod_signal = tod_signal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "sacred-crime",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training set from history file\n",
    "df = pd.read_csv('vantaa-2017-2020.csv.gz', compression='gzip')\n",
    "df['time'] = pd.to_datetime(df[['Pv','Kk','Vuosi', 'Klo']]\n",
    "                   .astype(str).apply(' '.join, 1), format='%d %m %Y %H:%M')\n",
    "df.drop(columns=['Pv','Kk','Vuosi', 'Klo', 'Aikavyöhyke'], inplace=True)\n",
    "df.index = df['time']\n",
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "breathing-murray",
   "metadata": {},
   "outputs": [],
   "source": [
    "interpo = df.resample('600s').mean().interpolate()\n",
    "interpo.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "subsequent-perspective",
   "metadata": {},
   "outputs": [],
   "source": [
    "wind_direction = interpo['Tuulen suunta (deg)']\n",
    "wind_v = interpo['Tuulen nopeus (m/s)']\n",
    "wind_v_max = interpo['Puuskanopeus (m/s)']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "greek-worcester",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.hist2d(wind_direction, wind_v, bins=(50, 50), vmax=400)\n",
    "plt.colorbar()\n",
    "plt.xlabel('Wind Direction [deg]')\n",
    "plt.ylabel('Wind Velocity [m/s]')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "applicable-delivery",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert to wind vector\n",
    "interpo.wind_vector(interpo.pop('Tuulen nopeus (m/s)'), interpo.pop('Puuskanopeus (m/s)'), interpo.pop('Tuulen suunta (deg)'))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "pointed-soldier",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.hist2d(interpo['Wx'], interpo['Wy'], bins=(50, 50), vmax=400)\n",
    "plt.colorbar()\n",
    "plt.xlabel('Wind X [m/s]')\n",
    "plt.ylabel('Wind Y [m/s]')\n",
    "ax = plt.gca()\n",
    "ax.axis('tight')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "french-verse",
   "metadata": {},
   "outputs": [],
   "source": [
    "# look for seasonality although we know that year and day will be important features for weather data\n",
    "fft = tf.signal.rfft(interpo['Ilman lämpötila (degC)'])\n",
    "f_per_dataset = np.arange(0, len(fft))\n",
    "\n",
    "n_samples_h = len(interpo['Ilman lämpötila (degC)'])\n",
    "samples_per_year = 6*24*365.2524 # 10m sampling rate\n",
    "years_per_dataset = n_samples_h/(samples_per_year)\n",
    "\n",
    "f_per_year = f_per_dataset/years_per_dataset\n",
    "plt.step(f_per_year, np.abs(fft))\n",
    "plt.xscale('log')\n",
    "plt.ylim(0, 2000000)\n",
    "plt.xlim([0.1, max(plt.xlim())])\n",
    "plt.xticks([1, 365.2524], labels=['1/Year', '1/day'])\n",
    "_ = plt.xlabel('Frequency (log scale)')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "center-gothic",
   "metadata": {},
   "outputs": [],
   "source": [
    "# calculate time of day signal from time index\n",
    "interpo.tod_signal(interpo.index.get_level_values('time'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "pleasant-battle",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(np.array(interpo['Day sin'])[:25*6])\n",
    "plt.plot(np.array(interpo['Day cos'])[:25*6])\n",
    "plt.xlabel('Time step')\n",
    "plt.title('Time of day signal')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "considerable-turtle",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split dataset\n",
    "column_indices = {name: i for i, name in enumerate(interpo.columns)}\n",
    "\n",
    "n = len(interpo)\n",
    "train_df = interpo[0:int(n*0.7)]\n",
    "val_df = interpo[int(n*0.7):int(n*0.9)]\n",
    "test_df = interpo[int(n*0.9):]\n",
    "\n",
    "num_features = interpo.shape[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "necessary-provision",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Normalize\n",
    "train_mean = train_df.mean()\n",
    "train_std = train_df.std()\n",
    "train_df = (train_df - train_mean) / train_std\n",
    "val_df = (val_df - train_mean) / train_std\n",
    "test_df = (test_df - train_mean) / train_std"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "controversial-advance",
   "metadata": {},
   "outputs": [],
   "source": [
    "# store values used for normalization so that we can use them on predicitions\n",
    "out_path = Path('predict')\n",
    "out_path.mkdir(parents=True, exist_ok=True)\n",
    "train_mean.to_pickle('predict/train_mean.pkl')\n",
    "train_std.to_pickle('predict/train_std.pkl')\n",
    "pd.DataFrame(train_df.columns).to_csv('predict/trainset_columns.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "latter-playing",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_std = (interpo - train_mean) / train_std\n",
    "df_std = df_std.melt(var_name='Column', value_name='Normalized')\n",
    "plt.figure(figsize=(12, 6))\n",
    "ax = sns.violinplot(x='Column', y='Normalized', data=df_std)\n",
    "_ = ax.set_xticklabels(interpo.keys(), rotation=90)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "quarterly-interview",
   "metadata": {},
   "outputs": [],
   "source": [
    "class WindowGenerator():\n",
    "  def __init__(self, input_width, label_width, shift,\n",
    "               train_df=train_df, val_df=val_df, test_df=test_df,\n",
    "               label_columns=None):\n",
    "    # Store the raw data.\n",
    "    self.train_df = train_df\n",
    "    self.val_df = val_df\n",
    "    self.test_df = test_df\n",
    "\n",
    "    # Work out the label column indices.\n",
    "    self.label_columns = label_columns\n",
    "    if label_columns is not None:\n",
    "      self.label_columns_indices = {name: i for i, name in\n",
    "                                    enumerate(label_columns)}\n",
    "    self.column_indices = {name: i for i, name in\n",
    "                           enumerate(train_df.columns)}\n",
    "\n",
    "    # Work out the window parameters.\n",
    "    self.input_width = input_width\n",
    "    self.label_width = label_width\n",
    "    self.shift = shift\n",
    "\n",
    "    self.total_window_size = input_width + shift\n",
    "\n",
    "    self.input_slice = slice(0, input_width)\n",
    "    self.input_indices = np.arange(self.total_window_size)[self.input_slice]\n",
    "\n",
    "    self.label_start = self.total_window_size - self.label_width\n",
    "    self.labels_slice = slice(self.label_start, None)\n",
    "    self.label_indices = np.arange(self.total_window_size)[self.labels_slice]\n",
    "\n",
    "  def __repr__(self):\n",
    "    return '\\n'.join([\n",
    "        f'Total window size: {self.total_window_size}',\n",
    "        f'Input indices: {self.input_indices}',\n",
    "        f'Label indices: {self.label_indices}',\n",
    "        f'Label column name(s): {self.label_columns}'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "physical-mozambique",
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_window(self, features):\n",
    "  inputs = features[:, self.input_slice, :]\n",
    "  labels = features[:, self.labels_slice, :]\n",
    "  if self.label_columns is not None:\n",
    "    labels = tf.stack(\n",
    "        [labels[:, :, self.column_indices[name]] for name in self.label_columns],\n",
    "        axis=-1)\n",
    "\n",
    "  # Slicing doesn't preserve static shape information, so set the shapes\n",
    "  # manually. This way the `tf.data.Datasets` are easier to inspect.\n",
    "  inputs.set_shape([None, self.input_width, None])\n",
    "  labels.set_shape([None, self.label_width, None])\n",
    "\n",
    "  return inputs, labels\n",
    "\n",
    "WindowGenerator.split_window = split_window"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "mighty-simpson",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot(self, model=None, plot_col='Ilman lämpötila (degC)', max_subplots=3):\n",
    "  inputs, labels = self.example\n",
    "  plt.figure(figsize=(12, 8))\n",
    "  plot_col_index = self.column_indices[plot_col]\n",
    "  max_n = min(max_subplots, len(inputs))\n",
    "  for n in range(max_n):\n",
    "    plt.subplot(3, 1, n+1)\n",
    "    plt.ylabel(f'{plot_col} [normed]')\n",
    "    plt.plot(self.input_indices, inputs[n, :, plot_col_index],\n",
    "             label='Inputs', marker='.', zorder=-10)\n",
    "\n",
    "    if self.label_columns:\n",
    "      label_col_index = self.label_columns_indices.get(plot_col, None)\n",
    "    else:\n",
    "      label_col_index = plot_col_index\n",
    "\n",
    "    if label_col_index is None:\n",
    "      continue\n",
    "\n",
    "    plt.scatter(self.label_indices, labels[n, :, label_col_index],\n",
    "                edgecolors='k', label='Labels', c='#2ca02c', s=64)\n",
    "    if model is not None:\n",
    "      predictions = model(inputs)\n",
    "      plt.scatter(self.label_indices, predictions[n, :, label_col_index],\n",
    "                  marker='X', edgecolors='k', label='Predictions',\n",
    "                  c='#ff7f0e', s=64)\n",
    "\n",
    "    if n == 0:\n",
    "      plt.legend()\n",
    "\n",
    "  plt.xlabel('Time steps')\n",
    "\n",
    "WindowGenerator.plot = plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "framed-integral",
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_dataset(self, data):\n",
    "  data = np.array(data, dtype=np.float32)\n",
    "  ds = tf.keras.preprocessing.timeseries_dataset_from_array(\n",
    "      data=data,\n",
    "      targets=None,\n",
    "      sequence_length=self.total_window_size,\n",
    "      sequence_stride=1,\n",
    "      shuffle=True,\n",
    "      batch_size=32,)\n",
    "\n",
    "  ds = ds.map(self.split_window)\n",
    "\n",
    "  return ds\n",
    "\n",
    "WindowGenerator.make_dataset = make_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "known-contribution",
   "metadata": {},
   "outputs": [],
   "source": [
    "@property\n",
    "def train(self):\n",
    "  return self.make_dataset(self.train_df)\n",
    "\n",
    "@property\n",
    "def val(self):\n",
    "  return self.make_dataset(self.val_df)\n",
    "\n",
    "@property\n",
    "def test(self):\n",
    "  return self.make_dataset(self.test_df)\n",
    "\n",
    "@property\n",
    "def example(self):\n",
    "  \"\"\"Get and cache an example batch of `inputs, labels` for plotting.\"\"\"\n",
    "  result = getattr(self, '_example', None)\n",
    "  if result is None:\n",
    "    # No example batch was found, so get one from the `.train` dataset\n",
    "    result = next(iter(self.train))\n",
    "    # And cache it for next time\n",
    "    self._example = result\n",
    "  return result\n",
    "\n",
    "WindowGenerator.train = train\n",
    "WindowGenerator.val = val\n",
    "WindowGenerator.test = test\n",
    "WindowGenerator.example = example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "certain-georgia",
   "metadata": {},
   "outputs": [],
   "source": [
    "MAX_EPOCHS = 20\n",
    "\n",
    "def compile_and_fit(model, window, patience=2):\n",
    "  early_stopping = tf.keras.callbacks.EarlyStopping(monitor='val_loss',\n",
    "                                                    patience=patience,\n",
    "                                                    mode='min')\n",
    "\n",
    "  model.compile(loss=tf.losses.MeanSquaredError(),\n",
    "                optimizer=tf.optimizers.Adam(),\n",
    "                metrics=[tf.metrics.MeanAbsoluteError()])\n",
    "\n",
    "  history = model.fit(window.train, epochs=MAX_EPOCHS,\n",
    "                      validation_data=window.val,\n",
    "                      callbacks=[early_stopping])\n",
    "  return history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "small-mechanics",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now that we have defined the functions let's try multi step models to predict next 24h using on 24h history\n",
    "val_performance = {}\n",
    "performance = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "stupid-medium",
   "metadata": {},
   "outputs": [],
   "source": [
    "OUT_STEPS = 6*24\n",
    "multi_window = WindowGenerator(input_width=6*24,\n",
    "                               label_width=OUT_STEPS,\n",
    "                               shift=OUT_STEPS)\n",
    "\n",
    "multi_window.plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "linear-registration",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiStepLastBaseline(tf.keras.Model):\n",
    "  def call(self, inputs):\n",
    "    return tf.tile(inputs[:, -1:, :], [1, OUT_STEPS, 1])\n",
    "\n",
    "last_baseline = MultiStepLastBaseline()\n",
    "last_baseline.compile(loss=tf.losses.MeanSquaredError(),\n",
    "                      metrics=[tf.metrics.MeanAbsoluteError()])\n",
    "\n",
    "multi_val_performance = {}\n",
    "multi_performance = {}\n",
    "\n",
    "multi_val_performance['Last'] = last_baseline.evaluate(multi_window.val)\n",
    "multi_performance['Last'] = last_baseline.evaluate(multi_window.test, verbose=0)\n",
    "multi_window.plot(last_baseline)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "functional-communication",
   "metadata": {},
   "outputs": [],
   "source": [
    "# this baseline repeats the previous day assuming tomorrow is same as today\n",
    "class RepeatBaseline(tf.keras.Model):\n",
    "  def call(self, inputs):\n",
    "    return inputs\n",
    "\n",
    "repeat_baseline = RepeatBaseline()\n",
    "repeat_baseline.compile(loss=tf.losses.MeanSquaredError(),\n",
    "                        metrics=[tf.metrics.MeanAbsoluteError()])\n",
    "\n",
    "multi_val_performance['Repeat'] = repeat_baseline.evaluate(multi_window.val)\n",
    "multi_performance['Repeat'] = repeat_baseline.evaluate(multi_window.test, verbose=0)\n",
    "multi_window.plot(repeat_baseline)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "smaller-parker",
   "metadata": {},
   "outputs": [],
   "source": [
    "multi_linear_model = tf.keras.Sequential([\n",
    "    # Take the last time-step.\n",
    "    # Shape [batch, time, features] => [batch, 1, features]\n",
    "    tf.keras.layers.Lambda(lambda x: x[:, -1:, :]),\n",
    "    # Shape => [batch, 1, out_steps*features]\n",
    "    tf.keras.layers.Dense(OUT_STEPS*num_features,\n",
    "                          kernel_initializer=tf.initializers.zeros),\n",
    "    # Shape => [batch, out_steps, features]\n",
    "    tf.keras.layers.Reshape([OUT_STEPS, num_features])\n",
    "])\n",
    "\n",
    "history = compile_and_fit(multi_linear_model, multi_window)\n",
    "\n",
    "IPython.display.clear_output()\n",
    "multi_val_performance['Linear'] = multi_linear_model.evaluate(multi_window.val)\n",
    "multi_performance['Linear'] = multi_linear_model.evaluate(multi_window.test, verbose=0)\n",
    "multi_window.plot(multi_linear_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "informative-cambridge",
   "metadata": {},
   "outputs": [],
   "source": [
    "multi_lstm_model = tf.keras.Sequential([\n",
    "    # Shape [batch, time, features] => [batch, lstm_units]\n",
    "    # Adding more `lstm_units` just overfits more quickly.\n",
    "    tf.keras.layers.LSTM(8, return_sequences=False),\n",
    "    # Shape => [batch, out_steps*features]\n",
    "    tf.keras.layers.Dense(OUT_STEPS*num_features),\n",
    "                          #kernel_initializer=tf.initializers.zeros),\n",
    "    # Shape => [batch, out_steps, features]\n",
    "    tf.keras.layers.Reshape([OUT_STEPS, num_features])\n",
    "])\n",
    "\n",
    "history = compile_and_fit(multi_lstm_model, multi_window)\n",
    "\n",
    "IPython.display.clear_output()\n",
    "\n",
    "multi_val_performance['LSTM'] = multi_lstm_model.evaluate(multi_window.val)\n",
    "multi_performance['LSTM'] = multi_lstm_model.evaluate(multi_window.test, verbose=0)\n",
    "multi_window.plot(multi_lstm_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "annoying-delhi",
   "metadata": {},
   "outputs": [],
   "source": [
    "multi_lstm_model.save('multi_lstm')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "published-cincinnati",
   "metadata": {},
   "outputs": [],
   "source": [
    "class FeedBack(tf.keras.Model):\n",
    "  def __init__(self, units, out_steps):\n",
    "    super().__init__()\n",
    "    self.out_steps = out_steps\n",
    "    self.units = units\n",
    "    self.lstm_cell = tf.keras.layers.LSTMCell(units)\n",
    "    # Also wrap the LSTMCell in an RNN to simplify the `warmup` method.\n",
    "    self.lstm_rnn = tf.keras.layers.RNN(self.lstm_cell, return_state=True)\n",
    "    self.dense = tf.keras.layers.Dense(num_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "significant-gravity",
   "metadata": {},
   "outputs": [],
   "source": [
    "feedback_model = FeedBack(units=32, out_steps=OUT_STEPS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "qualified-champion",
   "metadata": {},
   "outputs": [],
   "source": [
    "def warmup(self, inputs):\n",
    "  # inputs.shape => (batch, time, features)\n",
    "  # x.shape => (batch, lstm_units)\n",
    "  x, *state = self.lstm_rnn(inputs)\n",
    "\n",
    "  # predictions.shape => (batch, features)\n",
    "  prediction = self.dense(x)\n",
    "  return prediction, state\n",
    "\n",
    "FeedBack.warmup = warmup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "broad-patent",
   "metadata": {},
   "outputs": [],
   "source": [
    "prediction, state = feedback_model.warmup(multi_window.example[0])\n",
    "prediction.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "simplified-jordan",
   "metadata": {},
   "outputs": [],
   "source": [
    "def call(self, inputs, training=None):\n",
    "  # Use a TensorArray to capture dynamically unrolled outputs.\n",
    "  predictions = []\n",
    "  # Initialize the lstm state\n",
    "  prediction, state = self.warmup(inputs)\n",
    "\n",
    "  # Insert the first prediction\n",
    "  predictions.append(prediction)\n",
    "\n",
    "  # Run the rest of the prediction steps\n",
    "  for n in range(1, self.out_steps):\n",
    "    # Use the last prediction as input.\n",
    "    x = prediction\n",
    "    # Execute one lstm step.\n",
    "    x, state = self.lstm_cell(x, states=state,\n",
    "                              training=training)\n",
    "    # Convert the lstm output to a prediction.\n",
    "    prediction = self.dense(x)\n",
    "    # Add the prediction to the output\n",
    "    predictions.append(prediction)\n",
    "\n",
    "  # predictions.shape => (time, batch, features)\n",
    "  predictions = tf.stack(predictions)\n",
    "  # predictions.shape => (batch, time, features)\n",
    "  predictions = tf.transpose(predictions, [1, 0, 2])\n",
    "  return predictions\n",
    "\n",
    "FeedBack.call = call"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "identical-scoop",
   "metadata": {},
   "outputs": [],
   "source": [
    "history = compile_and_fit(feedback_model, multi_window)\n",
    "\n",
    "IPython.display.clear_output()\n",
    "\n",
    "multi_val_performance['AR LSTM'] = feedback_model.evaluate(multi_window.val)\n",
    "multi_performance['AR LSTM'] = feedback_model.evaluate(multi_window.test, verbose=0)\n",
    "multi_window.plot(feedback_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "immediate-kentucky",
   "metadata": {},
   "outputs": [],
   "source": [
    "x = np.arange(len(multi_performance))\n",
    "width = 0.3\n",
    "\n",
    "\n",
    "metric_name = 'mean_absolute_error'\n",
    "metric_index = multi_lstm_model.metrics_names.index('mean_absolute_error')\n",
    "val_mae = [v[metric_index] for v in multi_val_performance.values()]\n",
    "test_mae = [v[metric_index] for v in multi_performance.values()]\n",
    "\n",
    "plt.bar(x - 0.17, val_mae, width, label='Validation')\n",
    "plt.bar(x + 0.17, test_mae, width, label='Test')\n",
    "plt.xticks(ticks=x, labels=multi_performance.keys(),\n",
    "           rotation=45)\n",
    "plt.ylabel(f'MAE (average over all times and outputs)')\n",
    "_ = plt.legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "parental-holiday",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pyenv-tms-demo",
   "language": "python",
   "name": "tms-demo"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

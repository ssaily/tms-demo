{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "computational-hamburg",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import datetime\n",
    "\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import tensorflow as tf\n",
    "\n",
    "import matplotlib as mpl\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "from tqdm import tqdm\n",
    "\n",
    "from confluent_kafka import avro, Consumer, KafkaError, KafkaException\n",
    "from confluent_kafka.avro import CachedSchemaRegistryClient\n",
    "from confluent_kafka.avro.serializer.message_serializer import MessageSerializer as AvroSerde\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "frozen-belfast",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load the model first before we start filling GPU mem with other stuff\n",
    "# NOTE! If running on notebook env make sure you don't have other kernels consuming GPU mem\n",
    "saved_model = tf.keras.models.load_model('multi_lstm')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "italic-offense",
   "metadata": {},
   "outputs": [],
   "source": [
    "def wind_vector(self, velocity, max_velocity, direction):\n",
    "    # Convert to radians.\n",
    "    wd_rad = direction*np.pi / 180\n",
    "    self['Wx'] = velocity*np.cos(wd_rad)\n",
    "    self['Wy'] = velocity*np.sin(wd_rad)\n",
    "    self['max Wx'] = max_velocity*np.cos(wd_rad)\n",
    "    self['max Wy'] = max_velocity*np.sin(wd_rad)\n",
    "pd.DataFrame.wind_vector = wind_vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "casual-ticket",
   "metadata": {},
   "outputs": [],
   "source": [
    "def tod_signal(self, date_time):\n",
    "    day = 24*60*60\n",
    "    year = (365.2425)*day\n",
    "    timestamp_s = date_time.map(datetime.datetime.timestamp)\n",
    "    self['Day sin'] = np.sin(timestamp_s * (2 * np.pi / day))\n",
    "    self['Day cos'] = np.cos(timestamp_s * (2 * np.pi / day))\n",
    "    self['Year sin'] = np.sin(timestamp_s * (2 * np.pi / year))\n",
    "    self['Year cos'] = np.cos(timestamp_s * (2 * np.pi / year))\n",
    "pd.DataFrame.tod_signal = tod_signal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "offshore-utility",
   "metadata": {},
   "outputs": [],
   "source": [
    "schema_registry_config = {}\n",
    "with open('../tms-secrets/schema_registry_uri') as f:\n",
    "    schema_registry_config['url'] = f.read().rstrip('\\n')\n",
    "    \n",
    "schema_registry = CachedSchemaRegistryClient(schema_registry_config)\n",
    "avro_serde = AvroSerde(schema_registry)\n",
    "deserialize_avro = avro_serde.decode_message"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "temporal-prerequisite",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_client():    \n",
    "    \n",
    "    consumer_config = { \"group.id\": \"jhub-mac-5\",\n",
    "                        \"max.poll.interval.ms\": 20000,\n",
    "                        \"session.timeout.ms\": 10000,\n",
    "                        \"default.topic.config\": {\"auto.offset.reset\": \"earliest\"},\n",
    "                        \"security.protocol\": \"SSL\",\n",
    "                        \"ssl.ca.location\": \"../tms-secrets/processing/ca.pem\",\n",
    "                        \"ssl.certificate.location\": \"../tms-secrets/processing/service.cert\",\n",
    "                        \"ssl.key.location\": \"../tms-secrets/processing/service.key\"\n",
    "                       }\n",
    "    with open('../tms-secrets/kafka_service_uri') as f:\n",
    "        consumer_config['bootstrap.servers'] = f.read().rstrip('\\n')\n",
    "    \n",
    "    return Consumer(consumer_config)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "through-express",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_dict = []\n",
    "def consume_records():\n",
    "    client = create_client()\n",
    "    client.subscribe([\"observations.weather.multivariate\"])\n",
    "    i = 0\n",
    "    for i in tqdm(range(400000)):\n",
    "        msg = client.poll(15)\n",
    "        if msg is None: \n",
    "            continue\n",
    "        \n",
    "        if msg.error():\n",
    "            if msg.error().code() == KafkaError._PARTITION_EOF:\n",
    "                # End of partition event\n",
    "                sys.stderr.write('%% %s [%d] reached end at offset %d\\n' %\n",
    "                                 (msg.topic(), msg.partition(), msg.offset()))\n",
    "            elif msg.error():\n",
    "                raise KafkaException(msg.error())\n",
    "        else:\n",
    "            value = deserialize_avro(message=msg.value(), is_key=False)\n",
    "            dataset_dict.append(value)\n",
    "            pass\n",
    "    client.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "leading-somerset",
   "metadata": {},
   "outputs": [],
   "source": [
    "# input dateset from Kafka\n",
    "#consume_records()\n",
    "kafka_df = pd.json_normalize(dataset_dict)\n",
    "kafka_df['measuredTime'] = pd.to_datetime(kafka_df['measuredTime'] * 1000 * 1000)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b714bac4-4d00-40eb-97df-a0c4fc1c5c14",
   "metadata": {},
   "outputs": [],
   "source": [
    "kafka_df.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "egyptian-dressing",
   "metadata": {},
   "outputs": [],
   "source": [
    "# select features\n",
    "columns = ['roadStationId', 'measuredTime', 'measurements.ILMANPAINE', 'measurements.ILMAN_KOSTEUS', 'measurements.ILMA', 'measurements.TUULENSUUNTA', 'measurements.MAKSIMITUULI', 'measurements.KESKITUULI']\n",
    "kafka_df = kafka_df[columns]\n",
    "kafka_df.index = kafka_df['measuredTime']\n",
    "del kafka_df['measuredTime']\n",
    "kafka_df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "direct-burke",
   "metadata": {},
   "outputs": [],
   "source": [
    "# fill gaps\n",
    "kafka_interpo = kafka_df.groupby('roadStationId').resample('600s').mean().interpolate()\n",
    "del kafka_interpo['roadStationId']\n",
    "len(kafka_interpo.index.unique(level='roadStationId'))\n",
    "# drop weather stations that can't provide all needed features\n",
    "kafka_interpo = kafka_interpo.dropna()\n",
    "kafka_interpo = kafka_interpo.iloc[kafka_interpo.index.get_level_values(0) == 2052]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "introductory-somalia",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create wind vectors from velocity and direction\n",
    "kafka_interpo.wind_vector(kafka_interpo.pop('measurements.TUULENSUUNTA'), kafka_interpo.pop('measurements.MAKSIMITUULI'), kafka_interpo.pop('measurements.KESKITUULI'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "british-mainland",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.hist2d(kafka_interpo['max Wx'], kafka_interpo['max Wy'], bins=(50, 50), vmax=10)\n",
    "plt.colorbar()\n",
    "plt.xlabel('Wind X [m/s]')\n",
    "plt.ylabel('Wind Y [m/s]')\n",
    "ax = plt.gca()\n",
    "ax.axis('tight')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "maritime-willow",
   "metadata": {},
   "outputs": [],
   "source": [
    "# calculate time of day signal from time index\n",
    "kafka_interpo.tod_signal(kafka_interpo.index.get_level_values('measuredTime'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "educational-moses",
   "metadata": {},
   "outputs": [],
   "source": [
    "column_names = pd.read_csv('predict/trainset_columns.csv', index_col=0)\n",
    "kafka_interpo.columns = column_names['0'].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bacterial-ukraine",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Normalize\n",
    "#train_mean = pd.read_pickle('predict/train_mean.pkl')\n",
    "#train_std = pd.read_pickle('predict/train_std.pkl')\n",
    "#kafka_norm = (kafka_interpo - train_mean) / train_std\n",
    "#kafka_norm.shape\n",
    "\n",
    "# Normalize\n",
    "input_mean = kafka_interpo.mean()\n",
    "input_std = kafka_interpo.std()\n",
    "kafka_norm = (kafka_interpo - input_mean) / input_std\n",
    "kafka_norm.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "smart-state",
   "metadata": {},
   "outputs": [],
   "source": [
    "kafka_norm.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "controlling-supervisor",
   "metadata": {},
   "outputs": [],
   "source": [
    "HOURS = 24\n",
    "#inpslic = slice(-6 * HOURS,None)\n",
    "inputdf = kafka_norm[:6 * HOURS]\n",
    "data = np.array(inputdf, dtype=np.float32)\n",
    "input = tf.keras.preprocessing.timeseries_dataset_from_array(\n",
    "      data=data,\n",
    "      targets=None,\n",
    "      sequence_length=len(inputdf),\n",
    "      sequence_stride=1,\n",
    "      shuffle=True,\n",
    "      batch_size=32,)\n",
    "y = saved_model.predict(input);\n",
    "#y = repeat_baseline.predict(input);\n",
    "\n",
    "result = pd.DataFrame(y[0,:], columns=kafka_norm.columns)\n",
    "result = result[:6 * HOURS]\n",
    "result = input_std * result + input_mean\n",
    "#result = train_std * result + train_mean\n",
    "\n",
    "result['date'] = inputdf.index.get_level_values(1) + pd.Timedelta(value = HOURS, unit = 'h')\n",
    "result = result.set_index('date').add_prefix('pred_')\n",
    "all = pd.concat([result, kafka_interpo.droplevel('roadStationId')], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fleet-praise",
   "metadata": {},
   "outputs": [],
   "source": [
    "mask = all.columns.str.contains('.*lämpötila.*|.*Suhteellinen.*')\n",
    "all.sort_index().loc[:,mask].plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3bc2caac-7890-4637-8fdf-b432a1f02be5",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pyenv-tms-demo",
   "language": "python",
   "name": "tms-demo"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
